version: '3.8'

services:
  # Backend API service (main application)
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-platform-local
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=true
      - DATABASE_URL=sqlite:///llm_optimization.db
      - SECRET_KEY=dev-secret-key-change-in-production
      - API_HOST=0.0.0.0
      - API_PORT=5000
      - LOG_LEVEL=INFO
      # Add your API keys here for full functionality
      # - OPENAI_API_KEY=your_openai_key_here
      # - ANTHROPIC_API_KEY=your_anthropic_key_here
    volumes:
      - ./models:/app/models
      - ./datasets:/app/datasets
      - ./logs:/app/logs
      - .:/app  # Mount source code for development
    ports:
      - "8000:5000"  # API Gateway (host:container)
      - "8501:8501"  # Streamlit (if needed)
    networks:
      - llm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Optional: Redis for enhanced caching (uncomment if needed)
  # redis:
  #   image: redis:7-alpine
  #   container_name: llm-platform-redis-local
  #   ports:
  #     - "6379:6379"
  #   networks:
  #     - llm-network
  #   restart: unless-stopped

networks:
  llm-network:
    driver: bridge